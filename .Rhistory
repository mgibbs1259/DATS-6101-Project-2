scale_color_manual(values = c("darkolivegreen3", "deepskyblue2"))
ggplot(df_avg_score_sum, aes(x = year, y = value, group = variable, colour = variable)) +
geom_line() +
ggtitle ("8th Grade Average Math Scores Sum") +
theme(plot.title = element_text(hjust = 0.5)) +
xlab("Year") +
ylab("Average Scores Sum") +
labs(color = "Variable") +
scale_color_manual(values = c("darkorchid3"))
#get sum of total revenue - total expenditure from 2005-2015
df$tr_minus_te <- df$total_revenue - df$total_expenditure
df_sum_tr_minus_te <- aggregate(list(df$tr_minus_te), by = list(df$year), FUN = sum)
colnames(df_sum_tr_minus_te) <- c("year", "sum_total_revenue-total_expenditure")
df_sum_tr_minus_te <- melt(df_sum_tr_minus_te, id.vars = "year")
#line plot of sum of total revenue - total expenditure from 2005-2015
ggplot(df_sum_tr_minus_te, aes(x = year, y = value, group = variable, colour = variable)) +
geom_line() +
ggtitle ("Sum of Total Revenue - Total Expenditure") +
theme(plot.title = element_text(hjust = 0.5)) +
xlab("Year") +
ylab("USD (millions)") +
labs(color = "Variable") +
scale_color_manual(values = c("darkorange2"))
#describe data frame
describe(df[, 3:5])
#summarize data frame
summary(df)
#divide graph area into three columns
par(mfrow = c(1, 3))
#boxplots of total revenue, total expenditure, and 8th grade average math scores
boxplot(df$total_revenue, main = "Total Revenue", ylab = "USD (millions)")
boxplot(df$total_expenditure, main = "Total Expenditure", ylab = "USD (millions)")
boxplot(df$avg_score, main = "8th Grade Average Math Scores", ylab = "Average Score")
#total revenue and total expenditure have outliers
#remove total revenue and total expenditure outliers
tr_outliers <- boxplot(df$total_revenue, plot = FALSE)$out
te_outliers <- boxplot(df$total_expenditure, plot = FALSE)$out
df <- df[!df$total_expenditure %in% c(tr_outliers, te_outliers)]
#divide graph area into three columns
par(mfrow = c(1, 2))
#boxplots of total revenue (no outliers) and total expenditure (no outliers)
boxplot(df$total_revenue, main = "Total Revenue (No Outliers)", ylab = "USD (millions)")
boxplot(df$total_expenditure, main = "Total Expenditure (No Outliers)", ylab = "USD (millions)")
#total revenue and total expenditure have no outliers
#divide graph area into three columns
par(mfrow = c(1, 3))
#density plots of total revenue, total expenditure, and 8th grade average math scores
plot(density(df$total_revenue), main = "Total Revenue", xlab = "USD (millions)", ylab = "Frequency", col = "darkolivegreen3")
polygon(density(df$total_revenue), col = "darkolivegreen3")
plot(density(df$total_expenditure), main = "Total Expenditure", xlab = "USD (millions)", ylab = "Frequency", col = "deepskyblue2")
polygon(density(df$total_expenditure), col = "deepskyblue2")
plot(density(df$avg_score), main = "8th Grade Average Math Scores", xlab = "Average Score", ylab = "Frequency", col = "darkorchid3")
polygon(density(df$avg_score), col = "darkorchid3")
#total revenue and total expenditure have slight left skew, 8th grade average math scores looks normally distributed
#Shapiro-Wilk normality test
shapiro.test(df$total_revenue)
shapiro.test(df$total_expenditure)
shapiro.test(df$avg_score)
#total revenue, total expenditure, and 8th grade average math scores have p values < 0.05, not normally distributed
#scatter plot
ggplot(df, aes(x = total_expenditure, y = avg_score)) +
geom_point(shape = 1) +
geom_smooth(method = lm, se = FALSE, color = "goldenrod1") +
ggtitle("8th Grade Average Math Scores vs. Total Expenditure") +
theme(plot.title = element_text(hjust = 0.5)) +
xlab("Total Expenditure (USD (millions))") +
ylab("Average Score")
#linearity
#covariance matrix
df_cov <- cov(df[, 3:5], method = "pearson")
df_cov
#correlation matrix
df_cor <- cor(df[, 3:5], method = "pearson")
df_cor
#correlation plot
corrplot(df_cor, method = "square", addCoef.col = "black", number.digits = 4)
#all relationships are positive
#strong positive relationship between total revenue and total expenditure
#weak positive relationships between total revenue and 8th grade average math scores and total expenditure and 8th grade average math scores
#model
df_lm <- lm(avg_score ~ total_expenditure, data = df)
summary(df_lm)
#residuals - not normally distributed
#coefficients - total_expenditure p-value < 0.05, statistically significant relationship between education expenditure and 8th grade average math scores
#adjusted R-squared - 3.59% of the variance in 8th grade average math scores can be explained by education expenditure, poor model
#model assumptions
par(mfrow = c(2, 2))
plot(df_lm)
df_lm_res <- df_lm$residuals
describe(df_lm_res)
shapiro.test(df_lm_res)
#linearity - linearity, scatterplot shows linearity, residual vs fitted plot shows residuals skewed to left and spread around nearly horizontal line
#independence of residuals - independence of residuals, observations are independent
#normality of residuals - no normality of residuals, normal Q-Q plot shows that residuals follow straight line well, Shapiro-Wilk normality test shows p value < 0.05
#homoscedasticity of residuals - possible homoscedasticity of residuals, residual vs fitted plot shows residuals skewed to left and spread around slightly negatively sloped line
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2
library(ResourceSelection)
library(pROC)
library(pscl)
library(ISLR)
library(dplyr)
library(ROCR)
library(bestglm)
library(openintro)
# Chunk 3
#read in data
data <- email
#check data
str(email)
#ensure variable classified correctly
data$to_multiple <- as.factor(data$to_multiple)
data$cc <- as.numeric(data$cc)
data$viagra <- as.factor(data$viagra)
data$format <- as.factor(data$format)
data$re_subj <- as.factor(data$re_subj)
data$exclaim_subj <- as.factor(data$exclaim_subj)
data$spam <- as.factor(data$spam)
#check data
str(data)
# Chunk 4
#logistic regression
#features: to_multiple, cc, attach, dollar, winner, inherit, viagra, password, format, re_subj, and exclaim_subj
#response: spam
spamlogit1 <- glm(spam ~ to_multiple + cc + attach + dollar + winner + inherit + viagra + password + format + re_subj + exclaim_subj, family = binomial(link = 'logit'), data = data)
#results
summary.glm(spamlogit1)
#to_multiple, attach, dollar, winner, inherit, password, format, and re_subj are statistically significant (p-value < 0.05), contribute to model
#null deviance is 2437.2 and residual deviance is 1931.5, not a great reduction
#AIC is 1955.5
#convert results to %
exp(coef(spamlogit1))
# Chunk 5
#get 80% train
train <- data[1:3137, ]
#get 20% test
test <- data[3138:3921,  ]
# Chunk 6
#logistic regression
#features: to_multiple, attach, dollar, winner, inherit, password, format, and re_subj
#response: spam
spamlogit2 <- glm(spam ~ to_multiple + attach + dollar + winner + inherit + password + format + re_subj, family = binomial(link = 'logit'), data = train)
#results
summary(spamlogit2)
#to_multiple, attach, winner, password, format, and re_subj are statistically significant (p-value < 0.05), contribute to model
#null deviance is 1729.6 and residual deviance is 1338.0, not a great reduction
#AIC is 1356, better than model 1
#convert results to %
exp(coef(spamlogit2))
# Chunk 7
#logistic regression
#features: to_multiple, attach, winner, password, format, and re_subj
#response: spam
spamlogit3 <- glm(spam ~ to_multiple + attach + winner + password + format + re_subj, family = binomial(link = 'logit'), data = train)
#results
summary(spamlogit3)
#to_multiple, attach, winner, password, format, and re_subj are statistically significant (p-value < 0.05), contribute to model
#null deviance is 1729.6 and residual deviance is 1343.6, not a great reduction
#AIC is 1357.6, better than model 1, worse than model 2
#convert results to %
exp(coef(spamlogit3))
# Chunk 8
#model 2 prediction
predict_spamlogit2 <- predict.glm(spamlogit2, test, type = 'response')
# Chunk 9
#model 2 hit rate
per_like_spamlogit2 <- ifelse(predict_spamlogit2 > 0.5, 1, 0)
hit_rate_spamlogit2 <- mean(per_like_spamlogit2 == test$spam)
hit_rate_spamlogit2
#model 2 hit rate is 85.20%, okay
#model 2 ROC curve
roc_spamlogit2 <- prediction(predict_spamlogit2, test$spam)
perf_roc_spamlogit2 <- performance(roc_spamlogit2, measure = 'tpr', x.measure = 'fpr')
plot(perf_roc_spamlogit2)
#model 2 ROC curve is okay
#model 2 AUC
perf_auc_spamlogit2 <- performance(roc_spamlogit2, measure = 'auc')
perf_auc_spamlogit2
#model 2 AUC is 0.76, okay
#model 2 is okay at predicting spam
library(bestglm)
library(car)
library(caret)
library(corrplot)
library(dplyr)
library(ggplot2)
library(ISLR)
library(Metrics)
library(pROC)
library(pscl)
library(rattle)
library(RColorBrewer)
library(ResourceSelection)
library(rpart)
library(rpart.plot)
library(ROCR)
library(tree)
library(verification)
model3 <- glm(binary_result ~ time + sentimentClass + sentimentPositive + sentimentNegative + sentimentNeutral)
model3 <- glm(binary_result ~ time + sentimentClass + sentimentPositive + sentimentNegative + sentimentNeutralfamily = binomial(link = "logit"), data = train)
model3 <- glm(binary_result ~ time + sentimentClass + sentimentPositive + sentimentNegative + sentimentNeutralfamily,  binomial(link = "logit"), data = train)
model3 <- glm(binary_result ~ time + sentimentClass + sentimentPositive + sentimentNegative + sentimentNeutral, family =   binomial(link = "logit"), data = train)
library(bestglm)
library(car)
library(caret)
library(corrplot)
library(dplyr)
library(ggplot2)
library(ISLR)
library(Metrics)
library(pROC)
library(pscl)
library(rattle)
library(RColorBrewer)
library(ResourceSelection)
library(rpart)
library(rpart.plot)
library(ROCR)
library(tree)
library(verification)
#read in data
df <- read.csv("data/data.csv")
#remove X column
df$X <- NULL
#check data
str(df)
#change data types
df$binary_result <- as.factor(df$binary_result)
df$sentimentClass <- as.factor(df$sentimentClass)
df$urgency <- as.factor(df$urgency)
#check data
str(df)
head(df)
#correlation matrix
dfCorrMatrix <- cor(df[, c(4:7, 11:13, 15:18)], method = "pearson")
#correlation plot
corrplot(dfCorrMatrix, method = "square", addCoef.col = "black", number.cex = 7/ncol(df))
#create df w/o highly correlated features
dfLogModel <- df[, c(4:5, 8:16)]
library(bestglm)
library(car)
library(caret)
library(corrplot)
library(dplyr)
library(ggplot2)
library(ISLR)
library(Metrics)
library(pROC)
library(pscl)
library(rattle)
library(RColorBrewer)
library(ResourceSelection)
library(rpart)
library(rpart.plot)
library(ROCR)
library(tree)
library(verification)
#read in data
df <- read.csv("data/data.csv")
#remove X column
df$X <- NULL
#check data
str(df)
#change data types
df$binary_result <- as.factor(df$binary_result)
df$sentimentClass <- as.factor(df$sentimentClass)
df$urgency <- as.factor(df$urgency)
#check data
str(df)
head(df)
#correlation matrix
dfCorrMatrix <- cor(df[, c(4:7, 11:13, 15:18)], method = "pearson")
#correlation plot
corrplot(dfCorrMatrix, method = "square", addCoef.col = "black", number.cex = 7/ncol(df))
#create df w/o highly correlated features
dfLogModel <- df[, c(4:5, 8:16)]
# Chunk 1
library(bestglm)
library(car)
library(caret)
library(corrplot)
library(dplyr)
library(ggplot2)
library(ISLR)
library(Metrics)
library(pROC)
library(pscl)
library(rattle)
library(RColorBrewer)
library(ResourceSelection)
library(rpart)
library(rpart.plot)
library(ROCR)
library(tree)
library(verification)
# Chunk 2
#read in data
df <- read.csv("data/data.csv")
#remove X column
df$X <- NULL
#check data
str(df)
#change data types
df$binary_result <- as.factor(df$binary_result)
df$sentimentClass <- as.factor(df$sentimentClass)
df$urgency <- as.factor(df$urgency)
#check data
str(df)
head(df)
# Chunk 3
#correlation matrix
dfCorrMatrix <- cor(df[, c(4:7, 11:13, 15:18)], method = "pearson")
#correlation plot
corrplot(dfCorrMatrix, method = "square", addCoef.col = "black", number.cex = 7/ncol(df))
#create df w/o highly correlated features
dfLogModel <- df[, c(4:5, 8:16)]
# Chunk 4
#get 80% train
train <- dfLogModel[1:11088, ]
#get 20% test
test <- dfLogModel[11089:13860, ]
# Chunk 5
#full model
fullModel <- glm(binary_result ~ volume + open + provider + sentimentClass + sentimentNegative + sentimentNeutral + sentimentPositive + urgency + firstMentionSentence + bodySize, family = binomial(link = "logit"), data = train)
#full model summary
summary(fullModel)
#volume and open are statistically significant (p-value < 0.05), contribute to model
#null deviance is 15345 and residual deviance is 15233, terrible reduction
#AIC is 15281
#full model VIF
vif(fullModel)
#VIF < 5, no multicollinearity issues
# Chunk 6
#best model - commented out due to run time
#colnames(train) <- c("volume", "open", "y", "provider", "sentimentClass", "sentimentNegative", "sentimentNeutral", "sentimentPositive", "urgency", "firstMentionSentence", "bodySize")
#bestTrain <- train[, c("volume", "open", "provider", "sentimentClass", "sentimentNegative", "sentimentNeutral", "sentimentPositive", "urgency", "firstMentionSentence", "bodySize", "y")]
#logres.bglm <- bestglm(Xy = bestTrain, family = binomial, IC = "AIC", method = "exhaustive")
#logres.bglm$BestModels
#best model
bestModel <- glm(binary_result ~ volume + open + provider + sentimentNegative + firstMentionSentence, family = binomial(link = "logit"), data = train)
#best model summary
summary(bestModel)
#volume and open are statistically significant (p-value < 0.05), contribute to model
#null deviance is 15345 and residual deviance is 15235, terrible reduction
#AIC is 15271
#best model VIF
vif(bestModel)
#VIF < 5, no multicollinearity issues
# Chunk 7
#best model predicition
bestModelPrediction <- predict(bestModel, newdata = test, type = "response")
#CHECK THIS PART!, best model prediction probability distribution
data.frame(predictions = bestModelPrediction) %>%
ggplot(aes(x = predictions)) +
geom_histogram(bins = 50) +
labs(title = "histogram of predictions") +
theme_bw()
# Chunk 8
#best model hit rate
bestModelPerLike <- ifelse(bestModelPrediction > 0.5, 1, 0)
bestModelHitRate <- mean(bestModelPerLike == test$binary_result)
bestModelHitRate
#best model hit rate is 47.44%, terrible
# Chunk 9
#best model ROC curve
bestModelROCpred <- prediction(bestModelPrediction, test$binary_result)
bestModelROCperf <- performance(bestModelROCpred, measure = "tpr", x.measure = "fpr")
plot(bestModelROCperf, xlab = "false positive rate", ylab = "true positive rate")
#best ROC curve is terrible
#best model AUC
bestModelAUC <- performance(bestModelROCpred, measure = "auc")
bestModelAUC
#model 2 AUC is 0.4967, terrible
# Chunk 10
#build model
tree.results <- rpart(binary_result ~ time+sentimentClass+sentimentNegative+sentimentNeutral+sentimentPositive+bodySize+wordCount, data = training, control= rpart.control(cp=.005), method = "class")
plot(tree.results, uniform=TRUE, margin=.05)
text(tree.results)
summary(tree.results)
# Draw the decision tree using fancyRpart and text tree
fancyRpartPlot(tree.results)
plot(tree.results, uniform=TRUE, margin=.05)
text(tree.results)
#make predictions
tree_pred<- predict(tree.results, newdata=dfTest, type="class")
#misclassification table (the (0, 0) (1,1) diagonals are the correct classifications)
with(dfTest, table(tree_pred, binary_result))
#using the table, we'll find the error
error_value<-  (1021+629)/ 2771
error_value
library(bestglm)
library(car)
library(caret)
library(corrplot)
library(dplyr)
library(ggplot2)
library(ISLR)
library(Metrics)
library(pROC)
library(pscl)
library(rattle)
library(RColorBrewer)
library(ResourceSelection)
library(rpart)
library(rpart.plot)
library(ROCR)
library(tree)
library(verification)
install.packages("tree")
#read in data
df <- read.csv("data/data.csv")
#remove X column
df$X <- NULL
#check data
str(df)
#change data types
df$binary_result <- as.factor(df$binary_result)
df$sentimentClass <- as.factor(df$sentimentClass)
df$urgency <- as.factor(df$urgency)
#check data
str(df)
head(df)
setwd("~/Documents/GitHub/R/DATS 6101 Project 2")
#check data
str(df)
#read in data
df <- read.csv("data/data.csv")
#remove X column
df$X <- NULL
#check data
str(df)
#change data types
df$binary_result <- as.factor(df$binary_result)
#correlation matrix
dfCorrMatrix <- cor(df[, c(4:7, 11:13, 15:18)], method = "pearson")
#correlation plot
corrplot(dfCorrMatrix, method = "square", addCoef.col = "black", number.cex = 7/ncol(df))
#best model
bestModel <- glm(binary_result ~ volume + open + provider + sentimentNegative + firstMentionSentence, family = binomial(link = "logit"), data = train)
#correlation matrix
dfCorrMatrix <- cor(df[, c(4:7, 11:13, 15:18)], method = "pearson")
#correlation plot
corrplot(dfCorrMatrix, method = "square", addCoef.col = "black", number.cex = 7/ncol(df))
#create df w/o highly correlated features
dfLogModel <- df[, c(4:5, 8:16)]
#get 80% train
train <- dfLogModel[1:11088, ]
#get 20% test
test <- dfLogModel[11089:13860, ]
#full model
fullModel <- glm(binary_result ~ volume + open + provider + sentimentClass + sentimentNegative + sentimentNeutral + sentimentPositive + urgency + firstMentionSentence + bodySize, family = binomial(link = "logit"), data = train)
#full model summary
summary(fullModel)
#volume and open are statistically significant (p-value < 0.05), contribute to model
#null deviance is 15345 and residual deviance is 15233, terrible reduction
#AIC is 15281
#full model VIF
vif(fullModel)
#VIF < 5, no multicollinearity issues
str(df)
library(bestglm)
library(car)
library(caret)
library(corrplot)
library(dplyr)
library(ggplot2)
library(ISLR)
library(Metrics)
library(pROC)
library(pscl)
library(rattle)
library(RColorBrewer)
library(ResourceSelection)
library(rpart)
library(rpart.plot)
library(ROCR)
library(tree)
library(verification)
#read in data
df <- read.csv("data/data.csv")
#remove X column
df$X <- NULL
#check data
str(df)
#change data types
df$binary_result <- as.factor(df$binary_result)
df$sentimentClass <- as.factor(df$sentimentClass)
df$urgency <- as.factor(df$urgency)
#check data
str(df)
head(df)
#correlation matrix
dfCorrMatrix <- cor(df[, c(4:7, 11:13, 15:18)], method = "pearson")
#correlation plot
corrplot(dfCorrMatrix, method = "square", addCoef.col = "black", number.cex = 7/ncol(df))
#create df w/o highly correlated features
dfLogModel <- df[, c(4:5, 8:16)]
#full model
fullModel <- glm(binary_result ~ volume + open + provider + sentimentClass + sentimentNegative + sentimentNeutral + sentimentPositive + urgency + firstMentionSentence + bodySize, family = binomial(link = "logit"), data = train)
#full model summary
summary(fullModel)
#volume and open are statistically significant (p-value < 0.05), contribute to model
#null deviance is 15345 and residual deviance is 15233, terrible reduction
#AIC is 15281
#full model VIF
vif(fullModel)
#VIF < 5, no multicollinearity issues
#best model - commented out due to run time
colnames(train) <- c("volume", "open", "y", "provider", "sentimentClass", "sentimentNegative", "sentimentNeutral", "sentimentPositive", "urgency", "firstMentionSentence", "bodySize")
bestTrain <- train[, c("volume", "open", "provider", "sentimentClass", "sentimentNegative", "sentimentNeutral", "sentimentPositive", "urgency", "firstMentionSentence", "bodySize", "y")]
logres.bglm <- bestglm(Xy = bestTrain, family = binomial, IC = "AIC", method = "exhaustive")
