xlab("Year") +
ylab("USD (millions)") +
labs(color = "Variable") +
scale_color_manual(values = c("darkolivegreen3", "deepskyblue2"))
ggplot(df_avg_score_sum, aes(x = year, y = value, group = variable, colour = variable)) +
geom_line() +
ggtitle ("8th Grade Average Math Scores Sum") +
theme(plot.title = element_text(hjust = 0.5)) +
xlab("Year") +
ylab("Average Scores Sum") +
labs(color = "Variable") +
scale_color_manual(values = c("darkorchid3"))
#get sum of total revenue - total expenditure from 2005-2015
df$tr_minus_te <- df$total_revenue - df$total_expenditure
df_sum_tr_minus_te <- aggregate(list(df$tr_minus_te), by = list(df$year), FUN = sum)
colnames(df_sum_tr_minus_te) <- c("year", "sum_total_revenue-total_expenditure")
df_sum_tr_minus_te <- melt(df_sum_tr_minus_te, id.vars = "year")
#line plot of sum of total revenue - total expenditure from 2005-2015
ggplot(df_sum_tr_minus_te, aes(x = year, y = value, group = variable, colour = variable)) +
geom_line() +
ggtitle ("Sum of Total Revenue - Total Expenditure") +
theme(plot.title = element_text(hjust = 0.5)) +
xlab("Year") +
ylab("USD (millions)") +
labs(color = "Variable") +
scale_color_manual(values = c("darkorange2"))
#describe data frame
describe(df[, 3:5])
#summarize data frame
summary(df)
#divide graph area into three columns
par(mfrow = c(1, 3))
#boxplots of total revenue, total expenditure, and 8th grade average math scores
boxplot(df$total_revenue, main = "Total Revenue", ylab = "USD (millions)")
boxplot(df$total_expenditure, main = "Total Expenditure", ylab = "USD (millions)")
boxplot(df$avg_score, main = "8th Grade Average Math Scores", ylab = "Average Score")
#total revenue and total expenditure have outliers
#remove total revenue and total expenditure outliers
tr_outliers <- boxplot(df$total_revenue, plot = FALSE)$out
te_outliers <- boxplot(df$total_expenditure, plot = FALSE)$out
df <- df[!df$total_expenditure %in% c(tr_outliers, te_outliers)]
#divide graph area into three columns
par(mfrow = c(1, 2))
#boxplots of total revenue (no outliers) and total expenditure (no outliers)
boxplot(df$total_revenue, main = "Total Revenue (No Outliers)", ylab = "USD (millions)")
boxplot(df$total_expenditure, main = "Total Expenditure (No Outliers)", ylab = "USD (millions)")
#total revenue and total expenditure have no outliers
#divide graph area into three columns
par(mfrow = c(1, 3))
#density plots of total revenue, total expenditure, and 8th grade average math scores
plot(density(df$total_revenue), main = "Total Revenue", xlab = "USD (millions)", ylab = "Frequency", col = "darkolivegreen3")
polygon(density(df$total_revenue), col = "darkolivegreen3")
plot(density(df$total_expenditure), main = "Total Expenditure", xlab = "USD (millions)", ylab = "Frequency", col = "deepskyblue2")
polygon(density(df$total_expenditure), col = "deepskyblue2")
plot(density(df$avg_score), main = "8th Grade Average Math Scores", xlab = "Average Score", ylab = "Frequency", col = "darkorchid3")
polygon(density(df$avg_score), col = "darkorchid3")
#total revenue and total expenditure have slight left skew, 8th grade average math scores looks normally distributed
#Shapiro-Wilk normality test
shapiro.test(df$total_revenue)
shapiro.test(df$total_expenditure)
shapiro.test(df$avg_score)
#total revenue, total expenditure, and 8th grade average math scores have p values < 0.05, not normally distributed
#scatter plot
ggplot(df, aes(x = total_expenditure, y = avg_score)) +
geom_point(shape = 1) +
geom_smooth(method = lm, se = FALSE, color = "goldenrod1") +
ggtitle("8th Grade Average Math Scores vs. Total Expenditure") +
theme(plot.title = element_text(hjust = 0.5)) +
xlab("Total Expenditure (USD (millions))") +
ylab("Average Score")
#linearity
#covariance matrix
df_cov <- cov(df[, 3:5], method = "pearson")
df_cov
#correlation matrix
df_cor <- cor(df[, 3:5], method = "pearson")
df_cor
#correlation plot
corrplot(df_cor, method = "square", addCoef.col = "black", number.digits = 4)
#all relationships are positive
#strong positive relationship between total revenue and total expenditure
#weak positive relationships between total revenue and 8th grade average math scores and total expenditure and 8th grade average math scores
#model
df_lm <- lm(avg_score ~ total_expenditure, data = df)
summary(df_lm)
#residuals - not normally distributed
#coefficients - total_expenditure p-value < 0.05, statistically significant relationship between education expenditure and 8th grade average math scores
#adjusted R-squared - 3.59% of the variance in 8th grade average math scores can be explained by education expenditure, poor model
#model assumptions
par(mfrow = c(2, 2))
plot(df_lm)
df_lm_res <- df_lm$residuals
describe(df_lm_res)
shapiro.test(df_lm_res)
#linearity - linearity, scatterplot shows linearity, residual vs fitted plot shows residuals skewed to left and spread around nearly horizontal line
#independence of residuals - independence of residuals, observations are independent
#normality of residuals - no normality of residuals, normal Q-Q plot shows that residuals follow straight line well, Shapiro-Wilk normality test shows p value < 0.05
#homoscedasticity of residuals - possible homoscedasticity of residuals, residual vs fitted plot shows residuals skewed to left and spread around slightly negatively sloped line
library(corrplot)
library(data.table)
library(dplyr)
library(ggfortify)
library(ggplot2)
library(ggvis)
library(grid)
library(plyr)
library(psych)
library(stats)
library(VIM)
finances <- fread("finances.csv")
scores <- fread("scores.csv")
str(finances)
str(scores)
#choose columns of interest
finances <- finances[, c(1, 2, 4, 8)]
#join finances and scores on year and state
df <- merge(finances, scores, by = c("YEAR", "STATE"))
#make column names lowercase
df <- setnames(df, tolower(names(df)))
#select 2005-2015
df <- df[df$year > 2004 & df$year < 2016]
#get total revenue in USD (millions)) and total expenditure in USD (millions)
df$total_revenue <- df$total_revenue/1000000
df$total_expenditure <- df$total_expenditure/1000000
#remove rows with avg_score = "-", avg_score = "‡"
df <- df[df$avg_score != "—" & df$avg_score != "‡"]
#select mathematics
df <- df[df$test_subject == 'Mathematics']
#select 8th grade
df <- df[df$test_year == 8]
#change year, state, avg_score, and test_year data types
df$year <- as.factor(df$year)
df$state <- as.factor(df$state)
df$avg_score <- as.numeric(df$avg_score, digits = 7)
df$test_year <- as.character(df$test_year)
#check data frame
str(df)
#get total revenue sum, total expenditure sum, and 8th grade average math scores sum from 2005-2015
df_tr_te_sum <- aggregate(list(df$total_revenue, df$total_expenditure), by = list(df$year), FUN = sum)
colnames(df_tr_te_sum) <- c("year", "total_revenue_sum", "total_expenditure_sum")
df_tr_te_sum <- melt(df_tr_te_sum, id.vars = "year")
df_avg_score_sum <- aggregate(list(df$avg_score), by = list(df$year), FUN = sum)
colnames(df_avg_score_sum) <- c("year", "avg_score_sum")
df_avg_score_sum <- melt(df_avg_score_sum, id.vars = "year")
#line plot of total revenue sum, total expenditure sum, and 8th grade average math scores sum from 2005-2015
ggplot(df_tr_te_sum, aes(x = year, y = value, group = variable, colour = variable)) +
geom_line() +
ggtitle ("Total Revenue Sum and Total Expenditure Sum") +
theme(plot.title = element_text(hjust = 0.5)) +
xlab("Year") +
ylab("USD (millions)") +
labs(color = "Variable") +
scale_color_manual(values = c("darkolivegreen3", "deepskyblue2"))
ggplot(df_avg_score_sum, aes(x = year, y = value, group = variable, colour = variable)) +
geom_line() +
ggtitle ("8th Grade Average Math Scores Sum") +
theme(plot.title = element_text(hjust = 0.5)) +
xlab("Year") +
ylab("Average Scores Sum") +
labs(color = "Variable") +
scale_color_manual(values = c("darkorchid3"))
#get sum of total revenue - total expenditure from 2005-2015
df$tr_minus_te <- df$total_revenue - df$total_expenditure
df_sum_tr_minus_te <- aggregate(list(df$tr_minus_te), by = list(df$year), FUN = sum)
colnames(df_sum_tr_minus_te) <- c("year", "sum_total_revenue-total_expenditure")
df_sum_tr_minus_te <- melt(df_sum_tr_minus_te, id.vars = "year")
#line plot of sum of total revenue - total expenditure from 2005-2015
ggplot(df_sum_tr_minus_te, aes(x = year, y = value, group = variable, colour = variable)) +
geom_line() +
ggtitle ("Sum of Total Revenue - Total Expenditure") +
theme(plot.title = element_text(hjust = 0.5)) +
xlab("Year") +
ylab("USD (millions)") +
labs(color = "Variable") +
scale_color_manual(values = c("darkorange2"))
#describe data frame
describe(df[, 3:5])
#summarize data frame
summary(df)
#divide graph area into three columns
par(mfrow = c(1, 3))
#boxplots of total revenue, total expenditure, and 8th grade average math scores
boxplot(df$total_revenue, main = "Total Revenue", ylab = "USD (millions)")
boxplot(df$total_expenditure, main = "Total Expenditure", ylab = "USD (millions)")
boxplot(df$avg_score, main = "8th Grade Average Math Scores", ylab = "Average Score")
#total revenue and total expenditure have outliers
#remove total revenue and total expenditure outliers
tr_outliers <- boxplot(df$total_revenue, plot = FALSE)$out
te_outliers <- boxplot(df$total_expenditure, plot = FALSE)$out
df <- df[!df$total_expenditure %in% c(tr_outliers, te_outliers)]
#divide graph area into three columns
par(mfrow = c(1, 2))
#boxplots of total revenue (no outliers) and total expenditure (no outliers)
boxplot(df$total_revenue, main = "Total Revenue (No Outliers)", ylab = "USD (millions)")
boxplot(df$total_expenditure, main = "Total Expenditure (No Outliers)", ylab = "USD (millions)")
#total revenue and total expenditure have no outliers
#divide graph area into three columns
par(mfrow = c(1, 3))
#density plots of total revenue, total expenditure, and 8th grade average math scores
plot(density(df$total_revenue), main = "Total Revenue", xlab = "USD (millions)", ylab = "Frequency", col = "darkolivegreen3")
polygon(density(df$total_revenue), col = "darkolivegreen3")
plot(density(df$total_expenditure), main = "Total Expenditure", xlab = "USD (millions)", ylab = "Frequency", col = "deepskyblue2")
polygon(density(df$total_expenditure), col = "deepskyblue2")
plot(density(df$avg_score), main = "8th Grade Average Math Scores", xlab = "Average Score", ylab = "Frequency", col = "darkorchid3")
polygon(density(df$avg_score), col = "darkorchid3")
#total revenue and total expenditure have slight left skew, 8th grade average math scores looks normally distributed
#Shapiro-Wilk normality test
shapiro.test(df$total_revenue)
shapiro.test(df$total_expenditure)
shapiro.test(df$avg_score)
#total revenue, total expenditure, and 8th grade average math scores have p values < 0.05, not normally distributed
#scatter plot
ggplot(df, aes(x = total_expenditure, y = avg_score)) +
geom_point(shape = 1) +
geom_smooth(method = lm, se = FALSE, color = "goldenrod1") +
ggtitle("8th Grade Average Math Scores vs. Total Expenditure") +
theme(plot.title = element_text(hjust = 0.5)) +
xlab("Total Expenditure (USD (millions))") +
ylab("Average Score")
#linearity
#covariance matrix
df_cov <- cov(df[, 3:5], method = "pearson")
df_cov
#correlation matrix
df_cor <- cor(df[, 3:5], method = "pearson")
df_cor
#correlation plot
corrplot(df_cor, method = "square", addCoef.col = "black", number.digits = 4)
#all relationships are positive
#strong positive relationship between total revenue and total expenditure
#weak positive relationships between total revenue and 8th grade average math scores and total expenditure and 8th grade average math scores
#model
df_lm <- lm(avg_score ~ total_expenditure, data = df)
summary(df_lm)
#residuals - not normally distributed
#coefficients - total_expenditure p-value < 0.05, statistically significant relationship between education expenditure and 8th grade average math scores
#adjusted R-squared - 3.59% of the variance in 8th grade average math scores can be explained by education expenditure, poor model
#model assumptions
par(mfrow = c(2, 2))
plot(df_lm)
df_lm_res <- df_lm$residuals
describe(df_lm_res)
shapiro.test(df_lm_res)
#linearity - linearity, scatterplot shows linearity, residual vs fitted plot shows residuals skewed to left and spread around nearly horizontal line
#independence of residuals - independence of residuals, observations are independent
#normality of residuals - no normality of residuals, normal Q-Q plot shows that residuals follow straight line well, Shapiro-Wilk normality test shows p value < 0.05
#homoscedasticity of residuals - possible homoscedasticity of residuals, residual vs fitted plot shows residuals skewed to left and spread around slightly negatively sloped line
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2
library(ResourceSelection)
library(pROC)
library(pscl)
library(ISLR)
library(dplyr)
library(ROCR)
library(bestglm)
library(openintro)
# Chunk 3
#read in data
data <- email
#check data
str(email)
#ensure variable classified correctly
data$to_multiple <- as.factor(data$to_multiple)
data$cc <- as.numeric(data$cc)
data$viagra <- as.factor(data$viagra)
data$format <- as.factor(data$format)
data$re_subj <- as.factor(data$re_subj)
data$exclaim_subj <- as.factor(data$exclaim_subj)
data$spam <- as.factor(data$spam)
#check data
str(data)
# Chunk 4
#logistic regression
#features: to_multiple, cc, attach, dollar, winner, inherit, viagra, password, format, re_subj, and exclaim_subj
#response: spam
spamlogit1 <- glm(spam ~ to_multiple + cc + attach + dollar + winner + inherit + viagra + password + format + re_subj + exclaim_subj, family = binomial(link = 'logit'), data = data)
#results
summary.glm(spamlogit1)
#to_multiple, attach, dollar, winner, inherit, password, format, and re_subj are statistically significant (p-value < 0.05), contribute to model
#null deviance is 2437.2 and residual deviance is 1931.5, not a great reduction
#AIC is 1955.5
#convert results to %
exp(coef(spamlogit1))
# Chunk 5
#get 80% train
train <- data[1:3137, ]
#get 20% test
test <- data[3138:3921,  ]
# Chunk 6
#logistic regression
#features: to_multiple, attach, dollar, winner, inherit, password, format, and re_subj
#response: spam
spamlogit2 <- glm(spam ~ to_multiple + attach + dollar + winner + inherit + password + format + re_subj, family = binomial(link = 'logit'), data = train)
#results
summary(spamlogit2)
#to_multiple, attach, winner, password, format, and re_subj are statistically significant (p-value < 0.05), contribute to model
#null deviance is 1729.6 and residual deviance is 1338.0, not a great reduction
#AIC is 1356, better than model 1
#convert results to %
exp(coef(spamlogit2))
# Chunk 7
#logistic regression
#features: to_multiple, attach, winner, password, format, and re_subj
#response: spam
spamlogit3 <- glm(spam ~ to_multiple + attach + winner + password + format + re_subj, family = binomial(link = 'logit'), data = train)
#results
summary(spamlogit3)
#to_multiple, attach, winner, password, format, and re_subj are statistically significant (p-value < 0.05), contribute to model
#null deviance is 1729.6 and residual deviance is 1343.6, not a great reduction
#AIC is 1357.6, better than model 1, worse than model 2
#convert results to %
exp(coef(spamlogit3))
# Chunk 8
#model 2 prediction
predict_spamlogit2 <- predict.glm(spamlogit2, test, type = 'response')
# Chunk 9
#model 2 hit rate
per_like_spamlogit2 <- ifelse(predict_spamlogit2 > 0.5, 1, 0)
hit_rate_spamlogit2 <- mean(per_like_spamlogit2 == test$spam)
hit_rate_spamlogit2
#model 2 hit rate is 85.20%, okay
#model 2 ROC curve
roc_spamlogit2 <- prediction(predict_spamlogit2, test$spam)
perf_roc_spamlogit2 <- performance(roc_spamlogit2, measure = 'tpr', x.measure = 'fpr')
plot(perf_roc_spamlogit2)
#model 2 ROC curve is okay
#model 2 AUC
perf_auc_spamlogit2 <- performance(roc_spamlogit2, measure = 'auc')
perf_auc_spamlogit2
#model 2 AUC is 0.76, okay
#model 2 is okay at predicting spam
setwd("~/Documents/GitHub/R/DATS 6101 Project 2")
# Chunk 1
library(bestglm)
library(car)
library(caret)
library(caTools)
library(corrplot)
library(dplyr)
library(ggplot2)
library(ISLR)
library(MASS)
library(Metrics)
library(MLmetrics)
library(pROC)
library(pscl)
library(rattle)
library(RColorBrewer)
library(ResourceSelection)
library(rattle)
library(rpart)
library(rpart.plot)
library(ROCR)
library(tree)
library(verification)
# Chunk 2
#read in data
df <- read.csv("data.csv")
#remove X column
df$X <- NULL
#check data
str(df)
#change data types
df$binary_result <- as.factor(df$binary_result)
df$sentimentClass <- as.factor(df$sentimentClass)
df$urgency <- as.factor(df$urgency)
#check data
str(df)
# Chunk 3
#correlation matrix
dfCorrMatrix <- cor(df[, c(4:7, 11:13, 15:18)], method = "pearson")
#correlation plot
corrplot(dfCorrMatrix, method = "square", addCoef.col = "black", number.cex = 7/ncol(df))
#create df w/o highly correlated features
dfModel <- df[, c(4:5, 8:16)]
# Chunk 4
#get 80% train
train <- dfModel[1:11088, ]
#get 20% test
test <- dfModel[11089:13860, ]
# Chunk 5
#full model
fullModel <- glm(binary_result ~ volume + open + provider + sentimentClass + sentimentNegative + sentimentNeutral + sentimentPositive + urgency + firstMentionSentence + bodySize, family = binomial(link = "logit"), data = train)
#full model summary
summary(fullModel)
#volume and open are statistically significant (p-value < 0.05), contribute to model
#null deviance is 15345 and residual deviance is 15233, terrible reduction
#AIC is 15281
#full model VIF
vif(fullModel)
#VIF < 5, no multicollinearity issues
# Chunk 6
#stepwise model
stepModel <- fullModel %>% stepAIC(trace = TRUE)
#stepwise model summary
summary(stepModel)
#volume and open are statistically significant (p-value < 0.05), contribute to model
#null deviance is 15345 and residual deviance is 15237, terrible reduction
#AIC is 15271
#stepwise model VIF
vif(stepModel)
#VIF < 5, no multicollinearity issues
# Chunk 7
#stepwise model predicition
stepModelPrediction <- predict(stepModel, newdata = test, type = "response")
# Chunk 8
#stepwise model accuracy
stepModelPerLike <- ifelse(stepModelPrediction > 0.5, 1, 0)
stepModelAccuracy <- mean(stepModelPerLike == test$binary_result)
stepModelAccuracy
#stepwise model accuracy is 47.40%, terrible
# Chunk 9
#stepwise model ROC curve
stepModelROCpred <- prediction(stepModelPrediction, test$binary_result)
stepModelROCperf <- performance(stepModelROCpred, measure = "tpr", x.measure = "fpr")
plot(stepModelROCperf, xlab = "false positive rate", ylab = "true positive rate")
#stepwise model ROC curve is terrible
#stepwise model AUC
stepModelAUC <- performance(stepModelROCpred, measure = "auc")
stepModelAUC
#stepwise model AUC is 0.4922, terrible
# Chunk 10
#decision tree
set.seed(1)
tree <- rpart(binary_result ~ volume + open + provider + sentimentClass + sentimentNegative + sentimentNeutral + sentimentPositive + urgency + firstMentionSentence + bodySize, data = train, control = rpart.control(maxdepth = 6), method = "class")
#decision tree results
View(tree$cptable)
summary(tree)
#open and volume are important variables
#decision tree visualization
png("tree.png", width = 1000, height= 600)
post(tree, file = "", title= "Decision Tree")
dev.off()
# Chunk 11
#decision tree prediction
treePrediction <- predict(tree, newdata = test, type = "class")
# Chunk 12
#decision tree true positive and true negative
treePredictionTable <- table(test$binary_result, treePrediction)
treePredictionTable
#decision tree confusion matrix
confusionMatrix(treePrediction, test$binary_result)
#decision tree accuracy is 49.06%, terrible
#decision tree confusion matrix
confusionMatrix(treePrediction, test$binary_result)
setwd("~/Documents/GitHub/R/DATS 6101 Project 2")
# Chunk 1: setup
knitr::opts_chunk$set(echo = FALSE)
# Chunk 2
library(knitr)
library(kableExtra)
library(bestglm)
library(car)
library(caret)
library(corrplot)
library(dplyr)
library(ggplot2)
library(ISLR)
library(MASS)
library(Metrics)
library(MLmetrics)
library(pROC)
library(pscl)
library(rattle)
library(RColorBrewer)
library(ResourceSelection)
library(rattle)
library(rpart)
library(rpart.plot)
library(ROCR)
library(tree)
library(verification)
# Chunk 3
knitr::include_graphics('data.png')
# Chunk 4
#read in data
df <- read.csv('data.csv')
#remove X column
df$X <- NULL
#check data
str(df)
#change data types
df$binary_result <- as.factor(df$binary_result)
df$sentimentClass <- as.factor(df$sentimentClass)
df$urgency <- as.factor(df$urgency)
#check data
str(df)
# Chunk 5
head(df)%>%
kable()%>%
kable_styling(c("striped", "hover", "condensed", "responsive")) %>%
scroll_box(width = "100%")
# Chunk 6
#correlation matrix
dfCorrMatrix <- cor(df[, c(4:7, 11:13, 15:18)], method = "pearson")
#correlation plot
corrplot(dfCorrMatrix, method = "square", addCoef.col = "black", number.cex = 7/ncol(df))
# Chunk 7
#create df w/o highly correlated features
dfModel <- df[, c(4:5, 8:16)]
#get 80% train
train <- dfModel[1:11088, ]
#get 20% test
test <- dfModel[11089:13860, ]
# Chunk 8
#decision tree
set.seed(1)
tree <- rpart(binary_result ~ volume + open + provider + sentimentClass + sentimentNegative + sentimentNeutral + sentimentPositive + urgency + firstMentionSentence + bodySize, data = train, control = rpart.control(maxdepth = 6), method = "class")
#decision tree visualization
knitr::include_graphics('tree.png')
