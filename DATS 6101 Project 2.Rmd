---
title: "Project 2"
course: "DATS 6101 Introduction to Data Science"
authors: "Mary Gibbs and Peter Riley"
output: html_notebook
editor_options: 
  chunk_output_type: console
---

Packages
```{r}
library(ResourceSelection)
library(pROC)
library(pscl)
library(ISLR)
library(dplyr)
library(ROCR)
library(bestglm)
library(openintro)
library(caret)
library(ggplot2)
library(verification)
library(corrplot)
```

Data 
```{r}
#read in data 
df <- read.csv('data/data.csv') 
#remove X column
df$X <- NULL 
#check data
str(df)
#change data types
df$binary_result <- as.factor(df$binary_result)
df$sentimentClass <- as.factor(df$sentimentClass)
df$urgency <- as.factor(df$urgency)
#check data
str(df)
head(df)
```

Exploratory Data Analysis
```{r}
```
Logistic Regression Model 
```{r}
#split into training and testing data
trainIndex<- createDataPartition(df$binary_result, times=1, p=0.8, list=FALSE)
head(trainIndex)

dfTrain <- df[trainIndex,]
dfTest  <- df[-trainIndex,]

# find any linear combos in features
lin_comb <- findLinearCombos(dfTrain[,c(4:7,11:13,15:18)])
lin_comb$remove
lin_comb$linearCombos
# take set difference of feature names and linear combos to get rid of collinear variables
d <- setdiff(seq(1:ncol(dfTrain)), lin_comb$remove)
d
# remove linear combo columns
training <- dfTrain[, d]
logmod <- glm(binary_result ~ time+sentimentClass+sentimentNegative+sentimentNeutral+sentimentPositive+bodySize+wordCount, data = training, family = binomial(link = 'logit'))
summary(logmod)

```
Use model for predictions
```{r}
preds<- predict(logmod, newdata = dfTest, type = "response")

#look at probability distribution of predictions
data.frame(preds=preds) %>%
  ggplot(aes(x = preds)) + 
  geom_histogram(bins = 50) +
  labs(title = 'Histogram of Predictions') +
  theme_bw()
```
ROC Curve
```{r}
#In order to use the package we first have to set the prediction 
newpred <- prediction(preds,dfTest$binary_result)
#Next we want to measure true possitives which is "tpr" and also False Positives "fpr"
newpred.performance <- performance(newpred, measure = "tpr",x.measure = "fpr")
#then we plot these two measures
plot(newpred.performance)
#Looking pretty good, we can also get the AUC again using the performance function
AUC <- performance(newpred, measure = "auc")
AUC
```
Decision Tree
```{r}
library(tree)
library(rpart)
library(rpart.plot)
library(Metrics)
library(rattle)
library(RColorBrewer)


#build model
tree.results <- rpart(binary_result ~ time+sentimentClass+sentimentNegative+sentimentNeutral+sentimentPositive+bodySize+wordCount, data = training, control= rpart.control(cp=.005), method = "class")

plot(tree.results, uniform=TRUE, margin=.05)
text(tree.results)
summary(tree.results)
# Draw the decision tree using fancyRpart and text tree
fancyRpartPlot(tree.results)

plot(tree.results, uniform=TRUE, margin=.05)
text(tree.results)
#make predictions
tree_pred<- predict(tree.results, newdata=dfTest, type="class")

#misclassification table (the (0, 0) (1,1) diagonals are the correct classifications)
with(dfTest, table(tree_pred, binary_result))

#using the table, we'll find the error 
error_value<-  (1021+629)/ 2771
error_value


```

```{r}

```

