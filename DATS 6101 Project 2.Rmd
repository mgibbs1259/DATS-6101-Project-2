---
title: "Project 2"
course: "DATS 6101 Introduction to Data Science"
authors: "Mary Gibbs and Peter Riley"
output: html_notebook
editor_options: 
  chunk_output_type: console
---

Packages
```{r}
library(ResourceSelection)
library(pROC)
library(pscl)
library(ISLR)
library(dplyr)
library(ROCR)
library(bestglm)
library(openintro)
library(caret)
library(ggplot2)
library(verification)
library(corrplot)
```

Data 
```{r}
#read in data 
df <- read.csv('data/data.csv') 
#remove X column
df$X <- NULL 
#check data
str(df)
#change data types
df$binary_result <- as.factor(df$binary_result)
df$sentimentClass <- as.factor(df$sentimentClass)
df$urgency <- as.factor(df$urgency)
#check data
str(df)
head(df)

```

Exploratory Data Analysis
```{r}
```
Logistic Regression Model 
```{r}
#split into training and testing data
trainIndex<- createDataPartition(df$binary_result, times=1, p=0.8, list=FALSE)
head(trainIndex)

dfTrain <- df[trainIndex,]
dfTest  <- df[-trainIndex,]

# find any linear combos in features
lin_comb <- findLinearCombos(dfTrain[,c(4:7,11:13,15:18)])
lin_comb$remove
lin_comb$linearCombos
# take set difference of feature names and linear combos to get rid of collinear variables
d <- setdiff(seq(1:ncol(dfTrain)), lin_comb$remove)
d
# remove linear combo columns
training <- dfTrain[, d]
logmod <- glm(binary_result ~ sentimentClass+sentimentNegative+sentimentNeutral+sentimentPositive+bodySize+wordCount+open, data = training, family = binomial(link = 'logit'))
summary(logmod)

```
Use model for predictions
```{r}
preds<- predict(logmod, newdata = dfTest, type = "response")

#look at probability distribution of predictions
data.frame(preds=preds) %>%
  ggplot(aes(x = preds)) + 
  geom_histogram(bins = 50) +
  labs(title = 'Histogram of Predictions') +
  theme_bw()
```
ROC Curve
```{r}
#In order to use the package we first have to set the prediction 
newpred <- prediction(preds,dfTest$binary_result)
#Next we want to measure true possitives which is "tpr" and also False Positives "fpr"
newpred.performance <- performance(newpred, measure = "tpr",x.measure = "fpr")
#then we plot these two measures
plot(newpred.performance)
#Looking pretty good, we can also get the AUC again using the performance function
AUC <- performance(newpred, measure = "auc")
AUC
```
Decision Tree
```{r}
#load in necessary packages
library(tree)
library(rpart)
library(rpart.plot)
library(Metrics)
library(rattle)
library(RColorBrewer)


#build model
set.seed(1)
tree.results <- rpart(binary_result ~ open+sentimentNegative+sentimentNeutral+sentimentPositive+bodySize+wordCount, data = training, control= rpart.control(maxdepth = 6), parms= list(split= 'gini'), method = "class")

View(tree.results$cptable)
plot(tree.results, uniform=TRUE, margin=.05)
text(tree.results)
summary(tree.results)
# Draw the decision tree using fancyRpart and text tree
fancyRpartPlot(tree.results)

plot(tree.results, uniform=TRUE, margin=.05)
text(tree.results)
#make predictions on tree model
tree_pred<- predict(tree.results, newdata=dfTest, type="class")

#look at accuracy of model with the criterion of gini index
confusionMatrix(tree_pred, dfTest$binary_result)

#misclassification table (the (0, 0) (1,1) diagonals are the correct classifications
with(dfTest, table(tree_pred, binary_result))

#using the above table, we'll find the hit rate; true positive and true negative error rate
hit_rate<-  (731+829)/ 2771
hit_rate

#use rmse as another value to predict accuracy. in order to calculate rmse, the actual and predicted values have to be numeric
rmse_value<- rmse(as.numeric(dfTest$binary_result), as.numeric(tree_pred))
rmse_value

```

```{r}

```

